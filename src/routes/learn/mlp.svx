---
title: MLP Parameters and Training
blurb: |
    An overview on the control parameters of multi-layer perceptrons.
tags: 
    - mlp
    - multi-layer perceptron
    - supervised learning
    - neural network
    - machine learning
    - regression
    - classification
flair: article
---

<script>
    import IndentNote from '$lib/components/IndentNote.svelte';
    import Image from '$lib/components/Image.svelte';
</script>

This article provides guidance on using the neural networks found in the FluCoMa toolkit, including explanation of the parameters, intuition about how neural networks learn, and some recommendations for training. FluCoMa contains two neural network objects, the [MLPClassifier](/reference/mlpclassifier) and [MLPRegressor](/reference/mlpregressor). "MLP" stand for multi-layer perceptron, which is a type of neural network (more info [below](/#feed-forward)). "Classifier" and "Regressor" refer to the task that each MLP can be trained to do. 

<IndentNote type='pointer'>

For more information on the tasks [Classification](/reference/mlpclassifier) and [Regression](/reference/mlpregressor), visit the linked object references. 

</IndentNote>

## Supervised Learning

Both the MLPClassifier and MLPRegressor are supervised learning algorithms meaning that they learn from input-output example pairs. The MLPClassifier can learn to predict output labels (in a [LabelSet](/reference/labelset)) from the labels' paired input data points (in a [DataSet](/reference/dataset)). The MLPRegressor can learn to predict output data points from their paired input data points.

Supervised learning is contrasted with unsupervised learning which tries to find patterns in data that is not labeled or paired with other data. FluCoMa also has unsupervised learning algorithms such as [PCA](/reference/pca), [KMeans](/reference/kmeans), [MDS](/reference/mds), [UMAP](/reference/umap), and [Grid](/reference/grid).

## Classification and Regression: MLP vs. KNN

FluCoMa has another pair of objects that do classification and regression with supervised learning: the [KNNClassifier](/reference/knnclassifier) and [KNNRegressor](/reference/knnregressor). The KNN objects work quite differently from the MLP objects, each having their strengths and weaknesses. You can learn more about how KNNs work at their respective reference pages. The main differences to know are that: 
1. the flexibility of the MLP objects make them generally more capable of learning complex relationships between inputs and outputs, 
2. the MLP objects take much longer to `fit` and involve more parameters and options than the KNN objects, and 
3. the KNN objects will likely take longer to make predictions than the MLP objects, depending on the size of the dataset (although they're still quite quick!).

## The Training Process

### Training Data and Testing Data

Before training a neural network it may be important to separate the input-output example pairs into a **training set** and **testing set**. The **training set** will be used to do the actual training of the neural network and the **testing set** will be used after the training to test how well it is performing. This can be important to simulate how the neural network might perform in the actual use-case you're aiming for. For example, if you [train an MLPClassifier to distinguish oboe and trombone timbres](/learn/classification-neural-network) so that during a live performance it can know which instrument is sounding, you would train on a **training set** of audio analyses that have corresponding labels: either "trombone" or "oboe". Then, to test the effectiveness of the training, you would make predictions using a **testing set** that has both oboe and trombone examples which were not part of the training set. This simulates the future use-case: having the neural network make predictions when a trombone and oboe are on stage making sounds, the analyses of which it has never seen before.

One common way of creating training data and testing data is to start with one big dataset of the input-output example pairs. Then, randomly split off a certain portion of the pairs and set them aside as the testing set (depending on the amount of data, 20% might be a good amount to try). This could be achieved by setting aside some of the sound files (or other source material) you're planning to use, or by creating big [DataSet](/reference/dataset)s and/or [LabelSet](/reference/labelset)s and then splitting the data into two portions. Once you have separate training and testing data, train the neural network using the training set and when the training is complete, use the set-aside testing set to see how it is performing.

For some tasks it may not make sense to set aside testing data. If you're [using a neural network to control a synthesiser](/learn/regression-neural-network) and you're training it with only 5 or so points, setting aside any of the points would be eliminating one of the input-output associations you're hoping it will learn! You'll want to just use all the points to train the neural network. Creating training and testing data makes sense when you have a lot of data such that when it is randomly split, each portion will likely still maintain the input-output associations you want the neural network to learn.

### Overfitting

Using a testing set can help determine if a neural net is overfitting. Overfitting means that the neural network has learned to _really accurately_ make predictions on the data it was trained with, but as soon as it is shown data it has never seen before it performs quite poorly. If a neural network is able to perform well on both the training data and the testing data, it is likely not overfit.

<IndentNote type='pointer'>

For another strategy to prevent overfitting, see the `validation` parameter below.

</IndentNote>

## How does a neural network learn?

### Feed-forward

The internal structure of a multi-layer perceptron creates a "feedforward" network in which input values are received at input neurons, passed through sequential hidden layers of neurons which compute values that are passed on to the following layer, eventually arriving at output neurons which provide their values as the output of the neural network. These sequential layers are "fully connected", meaning that each neuron in one layer connects to every neuron in the following layer. These connections are weights that control how much each neuron's output influences the neurons it is connected to. The neurons (except in the input layer) also use a bias (an added offset) and an activation function to compute its output. 

The weights and biases are internal parameters that are not controlled at all by the user. During the training process (called [backpropagation](https://en.wikipedia.org/wiki/Backpropagation)) the neural networks adjusts its internal weights and biases in order to minimise the difference (called the error) between the output it guesses and the desired output that the user has paired with the given input.

<Image
src="211220_01_synth_control_sc.txt.jpg"
label="A multi-layer perceptron neural network that takes two inputs, has one hidden layer of seven neurons, and predicts ten outputs."
/>

### `fit`

The message that tells an MLP to train is `fit` (see more in the [parameters](/#parameters) section below). `fit` is synonymous with "train". Its terminology is similar to "finding a line of best fit" or "fitting a curve to data". `fit` takes two arguments: 1. the inputs the neural network (a [DataSet](/reference/dataset)) and 2. the outputs we want the neural network to learn to predict (a [LabelSet](/reference/labelset) for the MLPClassifer, a [DataSet](/reference/dataset) for the MLPRegressor). The neural network will know which inputs to associate with which outputs because the input-output example pairs will have the same identifier in their respective "set" container.

Training a neural network is an iterative process meaning that it does the same procedure many times (often thousands) making small adjustments each time until is performing to desirable a degree accuracy. During the training process, an MLP neural network uses all the data points from the input [DataSet](/reference/dataset) and predicts output guesses (either a label for MLPClassifier or a data point for MLPRegressor). At first these guesses will be quite wrong because the MLP begins in a randomised state. After each of these output guesses it checks what the _desired_ output is for the given input and measures how wrong the guess was. The MLPRegressor will then slightly adjust its internal state based on how wrong it was so that it can be more accurate the next time it makes a prediction on this input. After training on each data point thousands of times, these small adjustments enable the neural network to (hopefully!) make accurate predictions on the data points in the DataSet, as well as on data points it has never seen before.

### The "Error" (aka "Loss")

Because training the neural network is an iterative process, it is likely that you'll send the `fit` message to the neural network many times. Each additional time it receives `fit`, it continues training, picking up from wherever it left off (the internal state of the neural network is not reset). Each time it concludes a fitting it will return a number called the error (also known as the loss). The error is a measure of how inaccurate the neural network still is at performing the task you've asked it to learn (so it may need more training!). Generally speaking, lower is better because it means there is _less_ error, therefore the neural network is more accurately performing the task it is learning.

There isn't any way to objectively reason about how low of an error is "low enough", and "too low" might mean the neural network is overfitting. Instead, we can tell if the neural network is learning by watching how it changes over multiple fittings. If the error is going down then we can tell that the neural network is learning from the data it is training on (which is good). At some point the error will start to level out or "plateau", meaning that it is no longer decreasing. At this point the neural network isn't improving any more. It seems to have learned all it can from the data provided. At this point it would be good to start using the neural network to make predictions, or perhaps test it's performance on some testing data.

### Tweaking the MLP Parameters

As you are observing how the error changes over multiple fittings, it may be useful to change some of the neural network's parameters between fittings to see if it can achieve a lower error, or change how the error is decreasing. See the parameter descriptions below for more information about why and how you might tweak each of them.

Because the parameters `hidden`, `activation`, and `outputActivation` change the internal structure of the neural network, the internal state of the neural network is necessarily reset and any training that has been done will be lost. Even so, it may be a good idea to play with these parameters because they can have a significant impact on how well and how quickly the neural network is able to learn from your data. All the other parameters can be changed without losing the internal state. 

## Parameters

### `hidden`

A list of numbers that specifies the internal structure of the neural network. Each number in the list represents one hidden layer of the neural network, the value of which is the number of neurons in that layer. For example, 3 3 (which is the default) specifies two hidden layers with three neurons each. The number of neurons in the input and output layers are determined from the corresponding datasets when training begins.

### `activation`

An integer indicating which activation function each neuron in the hidden layer(s) will use. The default is 2 ('relu'). The options are<sup>1</sup>:
    - 0: "identity" uses f(x) = x
    - 1: "sigmoid" uses the logistic sigmoid function, f(x) = 1 / (1 + exp(-x))
    - 2: "relu" uses the rectified linear unit function, f(x) = max(x,0)
    - 3: "tanh" uses the hyperbolic tan function, f(x) = tanh(x)
    
### `outputActivation` (MLPRegressor Only)
An integer representing which activation function each neuron in the output layer will use. The options are the same as for `Activation`. The default is 0 ('identity').

  * `Batch Size`: The number of data points to use in between adjustments of the MLP's internal parameters. Higher values will speed up the training and smooth out the neural networks adjustments as it makes the adjustments based on batches of data points.
  * `Max Iterations`: How many epochs to train when `fit` is called on the object. An Epoch is consists of training on all the data points one time.
  * `Learn Rate`: A scalar for how drastically to adjust the internal parameters during training. It can be useful to begin at a relatively high value, such as 0.1 to try to quickly get the neural network in the general area of the solution. Then after a few fittings, decrease the learning rate to a smaller value, maybe 0.01, to slow down the adjustments and let the neural network hone in on a solution. This is the most important parameter to adjust while training a neural network. Different data will require different uses of learning rate. Experimenting with the learning rate for a given dataset is a good way to determine what might be best for a given task.
  * `Validation`: A percentage (represented as a decimal) of the data point to set aside and not use for training. Instead these points will be used after each epoch to check how the neural network is performing. If it is found to no longer be improving, training will stop. This is useful if there are many data points. With a very small dataset validation should be 0.
  * `Momentum`: A scalar to apply smoothing to the adjustments made to the internal parameters.
  * `Tap In`: Sets which layer the neural network will use as input. This can be used to access different parts of a trained neural network such as the encoder or decoder of an [autoencoder](https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726).
  * `Tap Out`: Sets which layer the neural network will output values from. This can be used to access different parts of a trained neural network such as the encoder or decoder of an [autoencoder](https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726).

## Methods / Messages

* `Fit`: Train the neural network to map between a source and target FluidLabelSet.
* `Predict`: Predict outputs for all the data points in a provided FluidDataSet.
* `Predict Point`: Predict an output for a single data point.
* `Clear`: Reset all the MLP's internal parameters to a randomised state. This will erase all the learning that the neural network has done.
