---
title: Dealing with Time
author: James
blurb: |
    Strategies for encoding and explaining time to a computer.
tags:
    - time
    - smoothing
    - descriptors
    - audio descriptors
    - statistics
flair: tutorial
featuredimage: /learn/for-featured-images/2D-corpus-explorer.png
---

<script>
    import NoiseShape from '$lib/widget/time/NoiseShape.svelte';
</script>

## Introduction

When we analyse sound with computers, the notion of time is critical. For humans, listening is an incredibly complex process that bundles many physiological and psychological elements into a relatively passive and automatic process. We can identify different perceptual features of the sound simultaneously, understand intentions behind sound production, trace musical elements that evolve over time and come to interpretations that connect different listening experiences together. Computer's have no such understanding of sound and by comparison are really quite stupid when you compare them to what a human can do. What they are especially good at though is _quantitative_ listening, and generating numbers that may or may not faithfully express the nuances of sound evolving over time. You might be thinking, "Hey! You just described an audio descriptor", and you would be right. This set of learn articles is going to reflect on the tension between human and machine listening and how that effects the way we encode musical representation, specifically _time_ in data.

After reading the articles and resources, we hope that you feel empowered...

## Time, Evolution and Data

The fact that sound evolves over time is an inescapable facet of how we listen to and interpret it. Think about all the different ways that a sound may or may not vary and how this contributes so strongly to it's nature, ability to elicit different responses as a listener. One neat entry point is to start to describe sound with _curves_ and _shapes_.

<NoiseShape />

<!-- Shape Stuff -->

See, this is pretty easy! However, it's far more simplistic than what we really do as humans when we analyse a sound through listening. As humans, we probably combine, layer and meld together multiple shapes simultaneously over different time scales. This is really the precipice of the issue of encoding the evolving nature of sound into data. How much is time is "enough" to be meaningful? Is all time equal? How do bits of time relate to other bits of time?

We've seen this kind of thinking manifest in a somewhat straightforward way for people who have begun to learn the FluCoMa toolkit, especially using the [Classifying Sounds Using a Neural Network](/learn/classification-neural-network) tutorial as a starting point. If you haven't already I suggest you go and at least watch the video, but to quickly summarise the plot for your understanding, this is what the tutorial asks you to do:

1. Play a sound.
2. Send that sound to an *audio descriptor* which will generate some numbers that change over time as the sound changes.
3. Press a button to "capture" the audio descriptor at a point in time and store it.

The goal of this is to teach a neural network to identify the "class" of a sound by providing it with labelled examples of audio descriptor data that was generated at a specific point in time while it was playing back. We can see Ted doing this in the video here:

https://youtu.be/cjk9oHw7PQg?t=888

When people are doing this the results can be quite satisfying. However, lots of people have asked: "how can I represent more than just a single frame of analysis?". Indeed, this is one of the problematic aspects of the approach taught in this tutorial – everytime that button is pressed we are capturing an incredibly brief period of time (around 10 milliseconds) and telling the computer that this piece of data is very important in identifying that class of sound. Of course, it's entirely possible that two sounds which are perceived as dissimilar to us, have some very similar brief moments in time.

This is really just the start of thinking about _time_ and the tutorial encapsulates a singular strategy for measuring the perceptual characteristics of a sound with audio descriptors and giving it to the computer to understand. In response to this, we've developed a series of [Learn Articles](/learn) that we think might provoke you to consider how you represent time through data, and how the evolution of sound can be captured and encoded into workflows with the FluCoMa toolkit.

[Smoothing](/learn/smoothing-data)
<!-- [Pitch Refinement](/learn/refining-pitch-analysis) -->
<!-- [LPT](/learn/lpt) -->




 
