---
title: PCA
blurb: Principal component analysis of a FluidDataSet
tags: 
    - data
    - dimension reduction
    - dimensionality reduction
    - mapping
    - statistics
    - preprocessing
flair: reference
category: Analyse Data
---

<script>
    import YouTube from '$lib/components/YouTube.svelte';
</script>

Principal Component Analysis (PCA) fits to a [DataSet](/reference/DataSet) to determine its _principal components_, each of which is a new axis through the data that maximises the variance, or "differences", within the Data. PCA can then transform the original DataSet or individual points to position them in relation to the principal components (i.e., "new axes") for better comparing how they differ from other points in the DataSet. PCA is often used for dimensionality reduction (see below) and is also useful for removing redundancy (i.e., correlation) and/or noise (i.e., dimensions that are uniformly distributed) from a DataSet.

For a great visual explanation of PCA, watch this Computerphile video:

<YouTube 
url={'TJdH6rPA-TI'}
/>

## Calculating the Principal Components

In order to calculate the principal components, PCA analyses the original DataSet and determines how to "rotate" the data so that an axis can be drawn through the data along which the most variance in the data can be seen (for a excellent visual explanation, see the YouTube video above). The new axis, or attribute, is a principal component of the DataSet. PCA continues the process of "drawing axes", through the data (always maximising variance) until all the principal components have been determined (PCA can compute as many principal components as there are dimensions in the original DataSet). In order to ensure that each principal component represents variance in a different direction within the data, all of the principal components will be orthogonal to each other (meaning they create right angles). This process orders the principal components (starting at 1) according to how much of the variance in the DataSet they are able to demonstrate (from demonstrating the most variance to the least). The lower ordered principal components are therefore _more useful_ in understanding the differences between the points in the DataSet.

## PCA for Dimensionality Reduction

Because PCA orders the principal components from those that represent the most variance in the data to the least, one can effectively perform dimensionality reduction by removing some of the higher ordered principal components (in the transformed DataSet) because they are less useful at representing the variance in the data. This allows the DataSet to have fewer dimensions, while retaining the aspects of the data that are more useful in understanding the differences between the data points. 

As this is often how PCA is used, FluidPCA allows the user to specify the number of output dimensions desired, so that `transform` operates as a dimensionality reduction algorithm, similar to [UMAP](/reference/umap) and [MDS](/reference/mds). After FluidPCA transforms a DataSet it returns the _explained variance_: the fraction (between 0 and 1) of the total variance in the DataSet accounted for using the reduced number of dimensions it has transformed the DataSet into. Values close to 1 mean that most of the variance in the original data is represented while number near 0 mean most of the variance in the original data is not represented in the transformed DataSet. When using PCA, knowing the explained variance is quite important. Reducing the number of dimensions while loosing most of the variance will likely be problematic further in the processes. Aiming to retain an explained variance of 0.99, 0.95, or 0.9 are common.
