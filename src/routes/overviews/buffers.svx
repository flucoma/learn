---
title: Buffers in FluCoMa
blurb: |
    An attempt to explain our rationale for using buffers as an interface in FluCoMa
tags: 
    - interface
    - data
    - beginner
---
## Introduction
If you've been playing with the FluCoMa objects for a while now, you might be wondering why you have to use buffers (or arrays in PureData) so much. There are many reasons that FluCoMa leverages buffers as pivotal interface and design feature, and while they add complexity to certain workflows they also solve many problems at the same time. You also might be confused, especially as a Max user, and we'd like to try and rationalise our choices here and provide some insight on dealing with the buffer interface.

## Rationale

### Cross Platform and Environment Agnostic
FluCoMa is developed for PureData, SuperCollider and Max. These environments are great at writing audio processing chains but often exhibit their own quirks and idiosyncracies when dealing with collections of multi-dimensional data. In Max you might be used to using the family of `zl` objects, `coll` or `dict`. In PureData you're likely to have used `array`. SuperCollider is perhaps even more open-ended, depending on if you want to do data processing on the server or in the language. For machine learning, machine listening and decomposition we often a need a place to store data as an _input_, and somewhere else to store the _output_ of a given process. With the wide range of idiomatic interfaces that already exist across these platforms we decided on using buffers and PureData arrays as relatively consistent data storage and processing container.

This makes development simpler and more abstracted. We don't have to develop algorithms for a specific environment, rather, we dictate how a "buffer" is dealt with in the code. This means that across all of the creative coding environments that we support there is a singular conceptual interface that is integrated well with the environment. Furthermore, this facilitates more easily developing future environments wrappers as long as we describe in the development process how a buffer is accessed.

### Fluidity of Workflow and Interface
Another reason to use buffers as a generic container for sound and data is that it allows you to leverage your programming skills in your environment of choice. Max users will perhaps be familiar with `poke~`, `peek~`, `waveform~` and `groove~`, which all offer ways to tap into the data you generate with machine learning as well as audition the results of decomposition processes. You aren't "locked in" to working with the results in a certain way, which supports accessing, modifying and transforming the data more easily. For example, [Timo Hoogland](https://www.timohoogland.com/) uses the predictions of the [MLPClassifier](/reference/mlpclassifier) to create wavetables that are then accessed at audio rate in Max.

Don't forget, the contents of a buffer can be written to disk, meaning you can store analysis, decomposition and segmentation results beyond the life of your environment of choice, as well as import the results between them freely.

### Efficiency and Overcoming Shortfalls
Buffers are also useful for overcoming the shortfalls of certain environments. We often deal with large collections of multi-dimensional data in machine learning and machine listening. This is potentially problematic. For example, Max lists are limited to 32767 items, whereas buffers are not constrained in the same way. In addition, buffers offer the possibility of having several channels of data meaning you can bundle more data into a single "container". SuperCollider and PureData users are perhaps more comfortable with the idea of an array holding both data and sound, so offering the buffer up as an interface for working with FluCoMa might feel more natural.


## Workflows
Because buffers are used so ubiquitously throughout the FluCoMa toolkit it can be hard to reorientate your relationship to them in your creative coding environment of choice. One good way to start, is to imagine the `buffer~` `Buffer()` or `array` as a generic container or _grid_ that holds numbers. It is sort of like a spreadsheet in that regard where we can look on the x-axis (samples) and the y-axis (channels) to locate data that has been put into the container.

### Audio Descriptors
All of the FluCoMa non-realtime audio descriptor objects in the toolkit use buffers or arrays as inputs and outputs. The input will contain a sound file, and the output will be a place to store the audio descriptors.

Let's look at this example of calculating the fundamental pitch using [FluidPitch](/reference/pitch).

### Decomposition
Objects that deal with decomposing sound in non-realtime will use buffers or arrays as inputs and outputs. The sound that you specify as the `source`, will be the input and depending on the object an output container will be specified.

**HPSS**

### Segmentation

**Noveltyslice**

### Dataset

**tobuffer and from buffer**

## Related Links

Audio Descriptors Video